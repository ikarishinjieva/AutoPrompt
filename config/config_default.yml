use_wandb: False
dataset:
    name: 'dataset'
    records_path: null
    initial_dataset: ''
    label_schema: ["Yes", "No"]
    max_samples: 50
    semantic_sampling: False # Change to True in case you don't have M1. Currently there is an issue with faiss and M1

annotator:
    method : 'llm_batch'
    config:
        instructions: ['Does this movie review reveal plot points, characters, the ending, world-building details, or other key information not yet publicly available for the movie? (Yes/No)',
'Would a viewer''s enjoyment of the movie be diminished if they read this review beforehand? (Yes/No)',
'Did the filmmakers intentionally withhold the information revealed in this review, intending for viewers to discover it organically during the movie? (Yes/No)',]
        aggregation_mode: 'exist'  #'majority_vote',  'exist', or 'all'. exist/all is working only in case label_schema: ["Yes", "No"]!
        estimator_config:
            llm:
                type: 'Azure'
                name: 'gpt-4o-2024-05-13'
    #            async_params:
    #                retry_interval: 10
    #                max_retries: 2
                model_kwargs: {"seed": 220}
            num_workers: 1
            prompt: 'prompts/predictor/prediction.prompt'
            mini_batch_size: 1  #change to >1 if you want to include multiple samples in the one prompt
            mode: 'annotation'

predictor:
    method : 'llm'
    config:
        llm:
            type: 'Azure'
            name: 'gpt-4o-2024-05-13'
#            async_params:
#                retry_interval: 10
#                max_retries: 2
            model_kwargs: {"seed": 220}
        num_workers: 1
        prompt: 'prompts/predictor/prediction.prompt'
        mini_batch_size: 1  #change to >1 if you want to include multiple samples in the one prompt
        mode: 'prediction'

meta_prompts:
    folder: 'prompts/meta_prompts_classification'
    num_err_prompt: 1  # Number of error examples per sample in the prompt generation
    num_err_samples: 2 # Number of error examples per sample in the sample generation
    history_length: 4 # Number of sample in the meta-prompt history
    num_generated_samples: 10 # Number of generated samples at each iteration
    num_initialize_samples: 10 # Number of generated samples at iteration 0, in zero-shot case
    samples_generation_batch: 10 # Number of samples generated in one call to the LLM
    num_workers: 5 #Number of parallel workers
    warmup: 4 # Number of warmup steps

eval:
    function_name: 'accuracy'
    num_large_errors: 4
    num_boundary_predictions : 0
    error_threshold: 0.5

llm:
    type: 'Azure'
    name: 'gpt-4o-2024-05-13' # This is the meta-prompt LLM, it should be a strong model. For example, using GPT-3.5 will cause an error in many cases.
    temperature: 0.8

stop_criteria:
    max_usage: 2 #In $ in case of OpenAI models, otherwise number of tokens
    patience: 10 # Number of patience steps
    min_delta: 0.01 # Delta for the improvement definition
